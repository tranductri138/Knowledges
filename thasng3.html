<h1>ThÃ¡ng 3 â€“ Deep Learning & Fine-tune LLM</h1>

<h2>ğŸ¯ Má»¥c tiÃªu thÃ¡ng</h2>
<ul>
  <li>Hiá»ƒu cÆ¡ báº£n vá» Deep Learning vÃ  kiáº¿n trÃºc Transformer.</li>
  <li>ThÃ nh tháº¡o PyTorch Ä‘á»ƒ train model.</li>
  <li>Biáº¿t fine-tune LLM open-source (LLaMA 3, Mistral, Qwen) vá»›i LoRA/QLoRA.</li>
  <li>HoÃ n thÃ nh chatbot chuyÃªn ngÃ nh dá»±a trÃªn dá»¯ liá»‡u riÃªng.</li>
</ul>

<h2>ğŸ“… Tuáº§n 1 â€“ Deep Learning cÆ¡ báº£n & PyTorch</h2>
<ul>
  <li><strong>Má»¥c tiÃªu:</strong> Náº¯m Ä‘Æ°á»£c ná»n táº£ng Deep Learning vÃ  sá»­ dá»¥ng PyTorch.</li>
  <li><strong>Ná»™i dung há»c:</strong>
    <ul>
      <li>Neural Network cÆ¡ báº£n: neuron, layer, activation function.</li>
      <li>Backpropagation & gradient descent.</li>
      <li>PyTorch cÆ¡ báº£n: Tensor, Dataset, DataLoader, nn.Module.</li>
      <li>Train model Ä‘Æ¡n giáº£n (MNIST).</li>
    </ul>
  </li>
  <li><strong>Thá»±c hÃ nh:</strong>
    <ol>
      <li>CÃ i PyTorch (GPU náº¿u cÃ³).</li>
      <li>Viáº¿t model MLP phÃ¢n loáº¡i áº£nh MNIST.</li>
      <li>Train, evaluate, vÃ  lÆ°u model.</li>
    </ol>
  </li>
  <li><strong>Káº¿t quáº£ cuá»‘i tuáº§n:</strong> Hiá»ƒu cÃ¡ch train model Deep Learning, thÃ nh tháº¡o PyTorch cÆ¡ báº£n.</li>
  <li><strong>TÃ i nguyÃªn:</strong> <a href="https://pytorch.org/tutorials/">PyTorch Tutorials</a></li>
</ul>

<h2>ğŸ“… Tuáº§n 2 â€“ Transformer & LLM Architecture</h2>
<ul>
  <li><strong>Má»¥c tiÃªu:</strong> Hiá»ƒu cÃ¡ch LLM hoáº¡t Ä‘á»™ng.</li>
  <li><strong>Ná»™i dung há»c:</strong>
    <ul>
      <li>Kiáº¿n trÃºc Transformer: Attention, Multi-head Attention, Feed-forward.</li>
      <li>Tokenization (BPE, SentencePiece).</li>
      <li>So sÃ¡nh GPT, BERT, LLaMA, Mistral.</li>
      <li>Hugging Face Transformers cÆ¡ báº£n.</li>
    </ul>
  </li>
  <li><strong>Thá»±c hÃ nh:</strong>
    <ol>
      <li>Load model BERT tá»« Hugging Face.</li>
      <li>Tokenize vÄƒn báº£n vÃ  cháº¡y inference.</li>
      <li>Viáº¿t script tÃ³m táº¯t vÄƒn báº£n báº±ng model pre-trained.</li>
    </ol>
  </li>
  <li><strong>Káº¿t quáº£ cuá»‘i tuáº§n:</strong> Hiá»ƒu cáº¥u trÃºc Transformer, biáº¿t load vÃ  cháº¡y model tá»« Hugging Face.</li>
  <li><strong>TÃ i nguyÃªn:</strong> <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
</ul>

<h2>ğŸ“… Tuáº§n 3 â€“ Fine-tune LLM vá»›i LoRA/QLoRA</h2>
<ul>
  <li><strong>Má»¥c tiÃªu:</strong> Tinh chá»‰nh LLM open-source cho dá»¯ liá»‡u riÃªng.</li>
  <li><strong>Ná»™i dung há»c:</strong>
    <ul>
      <li>LoRA & QLoRA (Low-Rank Adaptation) â€“ fine-tune tiáº¿t kiá»‡m tÃ i nguyÃªn.</li>
      <li>Chuáº©n bá»‹ dá»¯ liá»‡u huáº¥n luyá»‡n (JSON, Alpaca format).</li>
      <li>Sá»­ dá»¥ng transformers + peft Ä‘á»ƒ fine-tune.</li>
    </ul>
  </li>
  <li><strong>Thá»±c hÃ nh:</strong>
    <ol>
      <li>Chá»n model (LLaMA 3 8B, Mistral 7B, Qwen 7B).</li>
      <li>Chuáº©n bá»‹ dá»¯ liá»‡u Q&A chuyÃªn ngÃ nh (vÃ­ dá»¥: tÃ i liá»‡u cÃ´ng ty).</li>
      <li>Fine-tune model vá»›i QLoRA.</li>
      <li>LÆ°u model vÃ  test local báº±ng Ollama.</li>
    </ol>
  </li>
  <li><strong>Káº¿t quáº£ cuá»‘i tuáº§n:</strong> CÃ³ model LLM Ä‘Ã£ fine-tune cho dá»¯ liá»‡u riÃªng, cháº¡y Ä‘Æ°á»£c local.</li>
  <li><strong>TÃ i nguyÃªn:</strong> <a href="https://huggingface.co/docs/peft/index">PEFT Docs</a></li>
</ul>

<h2>ğŸ“… Tuáº§n 4 â€“ TÃ­ch há»£p LLM Fine-tune vÃ o Chatbot</h2>
<ul>
  <li><strong>Má»¥c tiÃªu:</strong> NÃ¢ng cáº¥p chatbot ná»™i bá»™ thÃ nh chatbot chuyÃªn ngÃ nh.</li>
  <li><strong>Ná»™i dung há»c:</strong>
    <ul>
      <li>Káº¿t há»£p RAG + LLM fine-tune.</li>
      <li>Tá»‘i Æ°u prompt cho model fine-tune.</li>
      <li>Triá»ƒn khai model vá»›i vLLM Ä‘á»ƒ tÄƒng tá»‘c.</li>
    </ul>
  </li>
  <li><strong>Thá»±c hÃ nh:</strong>
    <ol>
      <li>TÃ­ch há»£p model fine-tune vÃ o API Node.js.</li>
      <li>Káº¿t há»£p Qdrant Ä‘á»ƒ tÃ¬m kiáº¿m tÃ i liá»‡u.</li>
      <li>Tá»‘i Æ°u prompt Ä‘á»ƒ model tráº£ lá»i chÃ­nh xÃ¡c.</li>
      <li>Deploy API lÃªn server GPU.</li>
    </ol>
  </li>
  <li><strong>Káº¿t quáº£ cuá»‘i tuáº§n:</strong> Chatbot chuyÃªn ngÃ nh cháº¡y trÃªn model fine-tune, tráº£ lá»i chÃ­nh xÃ¡c hÆ¡n.</li>
  <li><strong>TÃ i nguyÃªn:</strong> <a href="https://docs.vllm.ai/en/latest/">vLLM Docs</a></li>
</ul>

<h2>ğŸš€ Mini Project cuá»‘i thÃ¡ng â€“ Chatbot chuyÃªn ngÃ nh</h2>
<ul>
  <li><strong>Má»¥c tiÃªu:</strong> Ãp dá»¥ng toÃ n bá»™ kiáº¿n thá»©c thÃ¡ng 3.</li>
  <li><strong>YÃªu cáº§u:</strong>
    <ul>
      <li>Input: cÃ¢u há»i chuyÃªn ngÃ nh (vÃ­ dá»¥: luáº­t, y táº¿, ká»¹ thuáº­t).</li>
      <li>Output: cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c dá»±a trÃªn dá»¯ liá»‡u huáº¥n luyá»‡n.</li>
      <li>CÃ³ thá»ƒ cháº¡y local hoáº·c cloud.</li>
    </ul>
  </li>
  <li><strong>Tech stack:</strong> Python + PyTorch + Hugging Face + PEFT + Qdrant + Node.js API.</li>
  <li><strong>Káº¿t quáº£:</strong> Chatbot chuyÃªn ngÃ nh cháº¡y trÃªn model fine-tune, cÃ³ thá»ƒ demo cho khÃ¡ch hÃ ng hoáº·c Ä‘Æ°a vÃ o portfolio.</li>
</ul>